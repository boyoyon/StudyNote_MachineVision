<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>奥行き</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -30px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 30px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>11章  奥行き</center></h1>
<p>
場面の中の各点のカメラからの距離を計算することはコンピュータビジョンシステムの重要な作業の一つである。輝度画像から奥行きの様な情報を抽出する一般的な方法は、既知の距離だけ離れた2つのカメラを使って画像対を撮影すると言うものである。別の方法としては、動いているカメラから2枚かそれ以上の画像を写すと言う方法が奥行き情報の計算で使われる。輝度画像とは異なり、各ピクセルの値がセンサーからその点までの距離に対応する画像はレンジ画像と呼ばれる。その様な画像はレンジ画像処理システムを使って直接写される。この様なレンジ画像を得るために使われる最も一般的な2つの原理はレーダーと三角測量である。奥行き情報を直接計算するこれらの方法だけでなく、陰影やテクスチャーと言った画像の手掛かりを使って2D輝度画像から3D情報を間接的に推定する事も出来る。これらの手法も本章で簡単に説明する。
</p>
<h2>11.1  立体画像処理</h2>
<p>
  立体鏡の幾何学を図11.1に示す。最も単純なモデルはx方向にベースライン距離bだけ離した2つの同一のカメラである。このモデルでは[2つの]画像平面は同一平面上にある。場面の中の特徴は画像平面の異なる場所にある2つのカメラから見られる。画像平面中の2つの特徴点の位置のずれは視差(disparity)と呼ばれる。カメラの焦点と場面中の特徴点を通る平面はエピポーラ平面と呼ばれる。エピポーラ平面と画像平面の交わりはエピポーラ線を定義する。図に示されたモデルでは1番目の画像の特徴点は全て2番目の画像でも同じ行に現れる。実際にはエピポーラ線の不整合による垂直視差もある。立体鏡アルゴリズムの形式化の多くでは垂直視差0を仮定している。12章では立体視カメラの校正における相対方向問題について述べる。
</p>
<center><img src="images/fig11_1.png"></center>
<p>
図11.1　両方のカメラから見える場面内の任意の点は画像対の点に投影され、共役対と呼ばれる。二つの点の位置の変位は視差(disparity)と呼ばれる
</p>
<p>
<strong>定義11.1</strong> 　共役対とは場面中の同一点がそれぞれの画像に投影された2つの点の事である。<br>
<strong>定義11.2</strong> 　視差とは2つの画像を重ねた場合の共役対の距離のことである。
</p>
<p>
  図11.1において、場面中の点 \(P\) は左右それぞれの画像の \(P_l,P_r\) に現れる。一般性を失わないので、座標系の原点が左側のカメラの焦点と一致すると仮定しよう。相似な三角形 \(PMC_l\)と \(P_lLC_l\) を比較すると次式が得られる。
\[
\frac{x}{z}=\frac{x_l^\prime}{f} \tag{11.1}
\]
同様に、相似な三角形 \(PNC_r\)、\(P_rRC_r\) から次式を得る。
\[
\frac{x-b}{z}=\frac{x_r^\prime}{f} \tag{11.2}
\]
2式を組み合わせると以下が得られる。
\[
z=\frac{bf}{(x_l^\prime-x_r^\prime)} \tag{11.3}
\]
従って画面中の各点の奥行きは対応する画像点の視差が分かることで復元できる。
</p>
<p>
  デジタル画像の離散的な性質のため、特別なアルゴリズムを使ってサブピクセル[ピクセル未満]の精度の視差を計算しない限り視差は整数値になることに注意して欲しい。従って与えられた一組のカメラパラメータに対して、場面中の与えられた点の奥行き計算精度は、視差が大きくなる様にベースライン距離 \(b\) を増やすことで高められる。しかしこの様な広角の立体視法は別の問題を引き起こす。例えばベースライン距離を大きくすると両方のカメラで見える場面中の点の範囲が減ってしまう。更に､たとえ両方のカメラから見える領域であっても遠近投影で生じる歪みのため一方の画像ともう一方の画像では異なって見え、共役点を識別することを困難にする。
</p>
<p>
　立体視を容易にする画像対の中の特徴を検出し照合する問題を議論する前に、任意の位置で任意の方向を向いているカメラの画像処理システムについて簡単に考察しておこう。
</p>
<h3>11.1.1  任意の位置で任意の方向を向いたカメラ</h3>
<p>
　2つのカメラが任意の場所で任意の向きを向いている場合でも、図11.2に示すように場面中の1つの点に対応する画像点は、画像平面とエピポーラ平面(場面中の点と2つのカメラの焦点を含んでいる)の交線に沿った所にある。この図からエピポーラ線はもはや画像の行と対応しないことは明らかである。
</p>
<center><img src="images/fig1_12.png"></center>
<p>
図11.2　任意の位置、向きの2つのカメラ。それでも場面内の点に対応する画像点はエピポーラ線上にある
</p>
<p>
  あるシステムではカメラの光軸が空間上のある点で交わる[様に設定される]。この場合、視差は輻輳角(vergence angle より目具合のこと)に対して相対的になる。図11.3に示す様に、任意の角度に対して視差が0になる面が存在する。この面より遠くにあるオブジェクトは0より大きい視差となり、この面より近くにあるオブジェクトは視差が0より小さくなる。ある領域内では視差は3つの集団に別れる。
\[
\begin{align}
+　d &\gt 0 \\
-　d &\lt 0 \\
0　d &= 0
\end{align}
\]
これらの集まりは照合のあいまいさを解決するために使われる。
</p>
<center><img src="images/fig1_13.png"></center>
<p>
図11.3　空間上の点に焦点を合わせたステレオ・カメラ。カメラの角度により視差が0となる空間上の表面が規定される
</p>
<p>
  ごく最近の研究ではより良い画像解析を容易にするため位置や方向、その他のカメラパラメータをダイナミックに制御する話題が述べられている。能動視覚システムとして知られるシステムでは、画像解析工程でカメラパラメータと動きがダイナミックに制御される。この様なシステムでは場面中の色々な点の奥行きを計算することが一般的な作業である。
</p>
<h2>11.2  立体視照合</h2>
<p> 
立体視技法の暗黙の仮定は、立体視画像から共役対を識別できると言う事である。しかし立体視画像から共役対を検出することは極めて挑戦的な研究課題であり対応問題として知られている。
</p>
<p>
  対応問題は次のように述べることが出来る。「左画像の各点に対応する点を右画像の中から探せ」。共役対となる点を各画像から1つずつ見つける為には、点の類似度を測定する必要がある。照合する点は周囲のピクセルとはっきり区別できるべきであることは明らかである。そうでなけば(例えば均質な領域の中の点など)、全ての点が十分一致してしまう。従って、立体視照合を行う前に照合可能な特徴を探し出す必要がある。エッジの特徴と領域の特徴の両方が立体視照合では使われてきた。
</p>
<p>
  照合では全ての画像ピクセルの内の部分集合が選ばれると言う事は、奥行きは特徴点に対してのみ計算される事を意味している。その他の点の奥行きは補間技法を用いて推定される。
</p>
<p>
  エピポーラ制約が共役対照合の照合空間を大幅に限定している事に注意して欲しい。しかし、測定誤差やカメラ位置や方向に不確定により、照合点は画像平面内の推定エピポーラ線上に厳密にのらない場合もある。この様な場合には近傍も照合する必要がある。
</p>
<h3>11.2.1  エッジ照合</h3>
<p>
初めに立体鏡のアルゴリズムを紹介する。これや類似のアルゴリズムの背景となる基本的な着想は、画像にフィルターを掛けることにより左右の画像から特徴は引き出され、エピポーラ線に沿って特徴が照合されると言うものである。ここでの議論ではエピポーラ線は画像の行に沿っているものとする。このアルゴリズムはガウス関数の一階導関数で検出されるエッジを使う。ガウス関数の勾配を使って計算されたエッジはノイズに対してより安定している。立体視アルゴリズムの工程は以下の様になる。
</p>
<p>
<div class="styleBullet">
<ul>
<li>1. ステレオ画像対の各画像に対して4種類のフィルター幅のガウシアンフィルターを掛ける。各フィルター幅は小さいフィルターの2倍の幅になる様にする。これは最少フィルターで畳み込みを繰り返すことで効率的に実行できる。
 </li><li>2. 行内のエッジ位置を計算する
 </li><li>3. 方向や強度を比較することにより粗い解像度でエッジを照合する。水平エッジが照合できないことは明らかである。
 </li><li>4. より精細なスケールで照合する事により視差推定の精度を上げる。 </li>
</ul>
</div>
</p>
<p>
  サブピクセルの解像度でエッジを計算することは奥行き情報の精度を改善することに注意して欲しい。照合工程を簡略化するため、一方の画像中の各特徴の照合探索は、もう一方の画像の対応するエピポーラ線付近の限られた範囲に沿って行われる。更にエッジの方向は \(30°\)ごとに記録され、照合では粗い角度を用いる。方向は粗く量子化された \(x,y\) の偏微分と、ルックアップテーブルの使用により効率的に計算される。またエッジの方向やコントラストの差、その外の対応する可能性のある特徴間の類似度の欠如の測定量により減点された合成基準を使って照合の可能性を計算する事も出来る。
</p>
<p>
  自動焦点カメラを使ってエッジを粗いスケールで照合した後、領域がだいたい視差0となる様にカメラアングルの調整を行い、再度細かいスケールで照合を行う。この調整は最大視差を減らすので、偽照合は減り、正確な視差を計算するために小サイズのフィルターを使ったとしても照合工程のスピードは上がる。照合工程は視差の近似値を決めるため粗いスケールから始めなければならない。粗いスケールではエッジが少ないので偽照合の可能性も小さい。
</p>
<h2>11.2  領域相関</h2>
<p>
立体照合におけるエッジベースの手法の重大な制約は、計算された奥行きの値は遮蔽エッジに沿っては意味を持たない事である。遮蔽エッジでは奥行きがうまく定義されない。これらのエッジに沿っては、奥行きは前景の遮蔽エッジの奥行きから背景の奥行きまでの間のどこかになる。特に曲がったオブジェクトでは遮蔽エッジはシルエット・エッジとなり、2つの画像の中で観察される曲線は同じ物理的なエッジには対応しない。
</p>
<center><img src="images/fig11_0.png"></center>
<p>
あいにく、オブジェクトが高いコントラストの非遮蔽エッジを持つか、他の特徴を持たない限り、画像平面中の強いエッジは遮蔽エッジに沿って検出される。従って奥行き復元の主要な問題の1つは、対応の候補として画像中に分散している多くの特徴を識別する事である。対応関係を求めて可能性のある特徴を探す為に開発された多くの方法の1つは両方の画像で注目点を識別し、領域相関法を使ってそれらの点を照合すると言うものである。
</p>
<h4>領域内で注目点を見つける</h4>
<p>
2つの画像の点照合では、簡単に識別できて照合できる点が必要である。均一な領域内の点は明らかに照合にとって良い候補ではない。注目点演算子(?interest operator)は変化の大きい領域を探す。画像の中にはその様な照合で使える分離された領域が十分にあると期待されている。ある点を中心としたウィンドウ内の全ピクセルを使って計算した色々な方向に沿った変動は、その点の色々な方向に沿った個別性の良い計量になる。方向性変動は以下で与えられる。
\[
\begin{align}
I_1 &=\sum_{(x,y)\in S}\{f(x,y)-f(x,y+1)\}^2 \tag{11.4} \\
\\
I_2 &=\sum_{(x,y)\in S}\{f(x,y)-f(x+1,y)\}^2 \tag{11.5} \\
\\
I_3 &=\sum_{(x,y)\in S}\{f(x,y)-f(x+1,y+1)\}^2 \tag{11.6} \\
\\
I_4 &=\sum_{(x,y)\in S}\{f(x,y)-f(x+1,y-1)\}^2 \tag{11.7}
\end{align}
\]
</p>
<center><img src="images/fig11_00.png"></center>
<p>
ここで \(S\) はウィンドウ内のピクセルを表している。一般的なウィンドウのサイズは5×5から11×11の範囲である。単純なエッジはエッジ方向には変動しないので、上の方向性変動の最小値は中心ピクセル \((x_c,y_c)\) における注目値として扱われる。これにより検討対象からエッジピクセルを取り除く。なぜなら一方の画像のエッジピクセルはもう一方の画像の同じエッジに沿った全てのピクセルと照合するであろうから、正確な視差を決めることが難しくなる(特にエッジがエピポーラ線に沿っている場合は)。こうして次式を得る。
\[
I(x_c, y_c)=\min(I_1, I_2, I_3, I_4) \tag{11.8}
\]
</p>
<p>
　最後に複数の隣接するピクセルが同じ特徴に対する注目点に選ばれない様にするため、特徴点は注目量が極大になる所が選ばれる。更に、もしこの極大値が予め決めた閾より大きいなら、その点は"良い"注目点である。
</p>
<p>
  一旦特徴が両方の画像で識別されると、それらは様々な方法によって照合できる。簡単な手法は、1番目の画像内の特徴を中心とした小さなウィンドウと、2番目の画像内の対応する可能性のある全ての特徴を中心とする同様のウィンドウで相関を計算する。最も相関の高い特徴は対応していると考えられる。明らかにエピポーラ制約を満たす特徴だけが考慮される。ある程度の垂直視差を許容する為、エピポーラ線に近い特徴も対応している可能性のある特徴の集合に入れられる。
</p>
<p>
　2つの画像 \(f_1,f_2\) を考えよう。照合される特徴点の対の視差を\((d_x,d_y)\) とする。そうすると特徴を中心とする2つの領域間の類似度の計量が、以下の様に定義される相関係数 \(r(d_x,d_y)\) で与えられる。
\[
r(d_x,d_y)=\frac{\sum_{(x,y)\in S}[f_1(x,y)-f_1][f_2(x+d_x,y+d_y)-\bar f_2]}{\sqrt{\sum_{(x,y)\in S}\Big[ f_1(x,y)-\bar f_1\Big]^2\sum_{(x,y)\in S}\Big[ f_2(x+d_x,y+d_y)-\bar f_2\Big]^2}} \tag{11.9}
\]
ここで \(\bar f_1\) と \(\bar f_2\) は比較される2つの領域内のピクセルの平均輝度であり、合計は特徴点を中心とする小ウィンドウ内の全てのピクセルに渡って行われる。
</p>
<p>
  上の式の輝度値の代わりに、各ピクセルでのしきい処理をした符号付きの勾配強度を使うことで相関の精度が改善する。これは平滑化していない2つの画像の各ピクセルで勾配強度を計算し、\(0\) 以上と \(0\) 以下の2つの閾を使って\(－1,0,1\) の3つの値に写像することで実行される。これにより画像は矩形波に変換され、より感度の高い相関を生じる。これを実行するなら相関の式で示した正規化の項は不要になり、\(r(d_x,d_y)\) は対応するピクセル値の積の合計に簡単化される。
</p>
<p>
  たいていの場合、特徴点に近くに対応する場面内の点の奥行きも近い。この発見的方法は14.3節で述べる繰り返し緩和法で利用される。
</p>
<p>
  前に述べた様に、特徴に基づいた立体視照合は、画像中の特徴点に対応する場面中の点の粗い奥行きマップを生じる。13章で述べるように表面を再構成するためには粗い奥行きマップに対して表面補間や表面近似を実行しなければならない。
</p>
<p>
  立体再構成における重大な困難の1つは注目点の選択である。その様な点は通常、輝度における高い局所変動に基づいて選ばれる。困ったことにその様な点は角やその外の面の不連続で頻繁に起こり、そこでは滑らかさの拘束条件が満たされていない。マシンビジョンの応用によっては、構造化された照明を使うことでこの問題は解決されている。照明パターンは表面に投影され、図11.4の様にそれがなければ滑らかな面にさえ交点を作り出す。
</p>
<center><img src="images/fig11_4.png"></center>
<p>
図11.4  照明パターンは表面に投影され、それが無ければ滑らかな表面に交点を作る
</p>
<h2>11.3  *** からの形状復元 (shape from X)</h2>
<p>
上で述べた立体視画像処理の他に～の形状復元技法として知られる様々な手法が輝度画像から形状情報を抽出するために開発されてきた。これらの手法の多くは各点での絶対的な奥行きではなく、局所的な面の方向を推定する。各オブジェクトの少なくても1点の実際の奥行きが判れば、局所的な面の方向を積分することで同一オブジェクト上の他の点の奥行きが計算される。このためこれらの手法は奥行き計算の間接法と呼ばれる。ここではこれらの手法の幾つかを簡単に述べ、それらがもっと詳細に記述されている章へのポインターを提供する。
</p>
<h3>照度差ステレオ  Photometric Stereo</h3>
<p>
照度差ステレオでは、3つの異なる方向からの光源を使って同一場面の3枚の画像が撮影される。3枚の画像を撮影している間、カメラと場面中のオブジェクトの両方が止まっている必要がある。場面中のオブジェクトの表面反射特性を知ることにより、3つの光源で照らされている点の局所的な面の方向が計算される。この手法は9章で詳述されている。光度測量立体視法の大きな利点の1つは、カメラと場面が完全に止まっているので3つの画像内の点は完全に合致する事である。従ってこの手法では対応問題で悩むことはない。この手法の主な欠点は間接法であることと、注意深く制御された照明による画像処理システムは利用するには実際的ではない事である。
</p>
<h3>陰影からの形状復元  Shape from Shading (濃淡解析)</h3>
<p>
陰影からの形状復元法では表面形状情報を復元するために画像輝度の変化(陰影)を利用する。これは画像内の各点 \((x',y')\) に対応する場面中の表面の方向を計算することでなされる。輻射測定原理により課される拘束に加えて、陰影からの形状復元法は表面方向パラメータを計算するため表面は滑らかであると言う仮定をする。この手法は9章で詳述されている。明らかに陰影からの形状復元法は奥行き計算における間接法である。更に、滑らかであると言う拘束条件が全ての点では満足されず、表面反射特性が正確には分かっていないと不正確な復元になる。
</p>
<h3>テクスチャーからの形状復元  Shape from Texture (テクスチャー解析)</h3>
<p>
密度やサイズ、方向といったテクスチャー特性における画像面の変動は、テクスチャーからの形状復元アルゴリズムで利用される手掛かりである。例えば、テクスチャー要素のプリミティブサイズの強度、方向の最大変化として定義されるテクスチャー勾配は表面の方向を決める。テクスチャー要素の形状変化(例えば円が楕円に見える)の量を定めることも面の方向を決めるために役立つ。おそらくは構造化された照明による(以下の節で述べる)規則的な直線格子のテクスチャーを持つ表面の画像から、消失点を見つけることにより方向が一意に決定できる。奥行き計算に対する間接法である事に加えて、テクスチャーからの形状復元法はテクスチャー要素やその特性の位置や量を正確に定める困難を抱えている。テクスチャーからの形状復元は7章で述べられている。
</p>
<h3>焦点からの形状復元  Shape from Focus</h3>
<p>
光学システムの被写体深度は有限なため(8章参照)、適正な奥行きにあるオブジェトだけが画像内で焦点が合っており、それ以外の奥行きにあるオブジェトは距離に比例してぶれる。このぶれ効果を利用したアルゴリズムが開発されている。焦点の合った画像を、カメラパラメータとカメラからのオブジェクトの距離で決まる点拡がり関数で畳み込みする事で画像をモデル化する。奥行きは画像内のぶれの量を見積もったり、既知の或いは推定した直線の広がり関数を使ったりする事で復元される。この様な復元問題は数学的には不良設定である。しかしある種の応用では、特に定性的な奥行き情報を必要とする応用では焦点からの形状復元法は役に立つ。
</p>
<h3>動きからの形状復元  Shape from Motion (SfM)</h3>
<p>
動いているカメラを使って静止した場面を撮影した場合、フレーム間の画像平面座標での変位はカメラから場面中の点までの距離に依存する。従ってこれは前の節で述べた立体画像処理と似ている。代わりに、動いているオブジェクトも静止したカメラで画像シーケンスを撮影することで運動視差を生じる。この様な視差もオブジェクト点の位置と速度に依存している。オブジェクトの構造と運動を復元する方法は14章で詳述する。
</p>
<h2>11.4 距離画像処理  Range Imaging</h2>
<p>
視野角内の各点の距離を測り2次元関数として記録するカメラを距離画像処理システムと呼び、その結果出来上がる画像を距離画像と呼ぶ。距離画像はまた奥行きマップとしても知られている。距離画像の例を図11.5に示す。
</p>
<center><img src="images/fig11_5.png"></center>
<p>
<center>図11.5: コーヒーマグの距離画像</center>
</p>
<p>
  距離画像処理で最も一般的に使われる2つの原理は三角測量とレーダーである。構造化照明システムは、マシンビジョンで広く使われるが、奥行きマップの計算に音響またはレーザーレンジファインダーを使う。
</p>
<h3>11.4.1  構造化照明処理  Structured Lighting</h3>
<p>
構造化照明処理を使う画像処理は既知の幾何学パターンの照明で照らされた場面を用いるシステムを当てにしている。簡単な点投影システムでは、ライトプロジェクターとカメラは図11.6に示す様にベースライン距離 \(b\) だけ離れている。オブジェト座標 \((x,y,z)\) は次式により測定された画像座標 \((x',y')\)、投影角 \(θ\) と関連付けられる。
\[
[x\;y\;z]=\frac{b}{f\cot\theta-x^\prime}[x^\prime\;y^\prime\; f] \tag{11.10}
\]
この様な三角測量システムの距離解像度は、角度 \(θ\) と画像の水平位置 \(x'\) を測る精度で決まる。
</p>
<center><img src="images/fig11_6.png"></center>
<p>
<center>図11.6　カメラを中心とした三角測量幾何学</center>
</p>
<p>
  全ての点の奥行きを計算するため、場面は2次元格子パターン内の1点が一時に照らされる。それから各点の奥行きが上の式を使って計算され2次元レンジ画像が得られる。その逐次的な性質のため、この方法は時間が掛かり、ダイナミックに変化する場面には適さない。場面内のオブジェクト表面に投影された照明パターンが光源から空間的に離れた所にあるカメラで撮られる。観察される画像の照明パターンには、パターンが投影されるオブジェクト表面の形や向きに依存して決まるゆがみが含まれている。これを図11.7に示す(図11.4でも見られる)。
</p>
<center><img src="images/fig11_7.png"></center>
<p>
<center>図11.7　ストライプ化された照明法の説明</center>
</p>
<p>
カメラで見られる照明パターンには不連続、方向や曲率の変化が含まれている事に注意して欲しい。画像平面内の任意の点に対応する3Dオブジェクトの座標は、カメラの視線と照明平面の交点を計算することで算出する事が出来る。完全なオブジェクトの記述を得るため、図に示す様に光源をパンする[左右に振る]か、オブジェクトをコンベヤーベルトに乗せて動かして複数の画像を得る。オブジェクトの異なる表面は、類似した空間属性を持つストライプ状の照明をまとめる事で決定される。
</p>
<p>
  ダイナミックに変化する状況では、場面全体をカバーする完全な画像の集合を取り込むシーケンスの間、ストライプ状の照明を投影する事は現実的ではない。完全な奥行きマップを取り込むために複数のストライプ状の照明を同時に投影した場合、奥行きの異なるオブジェクトの表面に起因してストライプの照合を間違える可能性がある。その様な場合には、ストライプを一意にコード化して投影する。例えば2進符号化法を使うと、わずか \(log_2N\) 回の投影で完全なデータの組が取り込まれる。ここで \((N-1)\) が全ストライプ数である。図11.8に \(N=8\) の場合で、この方法が示されている。
</p>
<center><img src="images/fig11_8.png"></center>
<p>
図11.8　二進符号化された構造化照明の説明。投影シーケンスが各ストライプの二進符号を決める
</p>
<p>
  \(7\) 本のストライプのそれぞれが \((001)\) から \((111)\) までの一意の2進コードを持っている。\(log_28＝3\) なので、3枚の画像を取り込めば良い。各画像は3ビット2進コードのビット位置 \(1,2,3\) で識別される。各ストライプ状の照明は、ストライプ自身のコードの画像に対応するビットが \(1\) の場合にのみONになる。例えばストライプ\(2 (010)\) は2番目の画像[の撮影時]のみONになり、ストライプ\(7 (111)\) は3枚の画像全て[の撮影時]でONになる。3枚の画像におけるストライプはこれで一意に識別できるようになり、ストライプの照合に曖昧さはなくなった。激しく変化する場面では、2進コードではなく色でコード化した画像が使われる。
</p>
<p>
  構造化照明法は場面の照明を簡単に制御できる工業用の応用で広く使われている。典型的な応用では、オブジェクトがコンベヤーベルトに載って照明面を通過し、ストライプ状の照明の画像にゆがみを作る。そしてビーム状の照明面でのオブジェクトの輪郭が計算される。この工程はオブジェクトの形状を復元するために一定間隔で繰り返される。
</p>
<p>
  構造化照明システムの主な欠点は光源またはカメラから見えないオブジェクトの位置に対するデータが得られないことである。
</p>
<h3>11.4.2  イメージングレーダー</h3>
<p>
レンジ画像処理の2番目の方法はイメージングレーダーである。飛行時発射パルスレーダーでは、オブジェクトまでの距離は電磁パルスの発射から受信までの時間差を観測して計算する。奥行き情報は振幅変調したビームの送受信間の位相差を検出したり、周波数変調したビームの送受信信号をコヒーレントに合成してビート周波数を検出したりすることで得られる。商用レーザービームイメージングシステムのいくつかは、これらの原理を使って作られている。
</p>
<p>
  レンジ画像は奥行き値を明確に示しているため便利である。明確な奥行き情報が手に入れば後の処理は楽であると信じられた時期もあった。しかしたとえ奥行き情報の助けがあったとしても、画像解釈の基本作業の困難は全て残っている事が明らかになってきた。
</p>
<h2>11.5 アクティブビジョン</h2>
<p>
たいていのコンピュータビジョンシステムは一定の特性を持つシステムで取り込まれたデータを当てにしている。これにはビデオカメラのような受動センシングシステムも、レーザーレンジファインダーシステムのような能動センシングシステムも含まれる。これらのデータ取り込みモードとは対照的に、データ取り込みのパラメータと特性が場面解釈システムによってダイナミックに制御されるアクティブビジョンシステムは知覚にとって決定的であると言われている。アクティブビジョンの概念は新しいものではない。生物学的システムはいつでも能動的な方法でデータを獲得する。アクティブビジョンシステムは受動センサーを使うかも知れないし、能動センサーを使うかも知れない。しかし、アクティブビジョンシステムでは、取り込んだデータが場面解釈作業を容易にする様に焦点、開口、輻輳(vergence)、照明と言ったセンサーの状態パラメータが制御される。アクティブビジョンは本質的には、場面からのパラメータと誤差を測定、計算する事で制御された知能的データ取り込み処理である。これらの場面依存で文脈依存のパラメータの正確な定義は画像システムや処理システムの特性だけでなく、それらの相互依存の十分な理解が必要とされる。アクティブビジョンは非常に活発に研究されている領域である。
</p>
    </body>
</html>