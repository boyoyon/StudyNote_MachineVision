<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>付録B</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1>付録B<br>統計的手法</h1>
<p>
マシンビジョンは、画像データから幾何学的な情報を推定することから、統計幾何学とも言えるだろう。本書では、正規分布や線形回帰といった統計学の基礎的な要素をいくつか用いており、それらは付録にまとめられている。また、モデル化されていない誤差に対して堅牢な測定アルゴリズムを定式化するために、統計学におけるいくつかの新しい理論も用いている。
<!-- 
Machine vision could be called statistical geometry, since vision involves esti-
mating geometrical information from image data. This book uses som basic
material from statistics, such as the normal distribution and linear regres-
sion, which are reproduced in this appendix. The book also uses some new
idcas in statistics for formulating measurement algorithms that are robust to
unmodeled errors.-->
</p>
<h2>B.1 測定誤差</h2>
<p>
センサーや測定手順には、3種類の性能パラメータがある。分解能または精度とは、センサーが報告できる値の最小の変化である。再現性とは、同じ量を繰り返し測定した場合の変動である。正確度とは、既知の真の値の測定値の変動である。正確度と再現性の関係は簡単に覚えることができる:
<!-- There are three types of performance parameters for a sensor or measurement,
procedure. Resolution or precision is the smallest change in the value that a
sensor can report. Repeatability is the variation in repeated measurements
of the same quantity. Accuracy is the variation in measurements of a known
true value. It is easy to remember the relationship between accuracy and
repcatability:-->
\[
正確度(Accuracy) = 再現性(Repeatability) + 較正(Calibration) \tag{B.1}
\]
誤差の構成要素間にも同様の関係がある:
<!-- There is a similar relationship between the components of error:-->
\[
誤差(Error) = 分散(Variance) + バイアス(Bias) \tag{B.2}
\]
分散は計測における再現性の誤差であり、バイアスは較正不足に起因する体系的な誤差である。例えば、廊下の長さをヤード棒で測定しているときに、ヤード棒の代わりに誤ってメートル棒を手に取ったとする。廊下の長さを測ったのと同じくらい注意深く測定棒を端から端まで置いていたと仮定すると、計測結果は依然として同じ再現性を持つ。しかし、測定棒の長さが不正確であるため、廊下の実際の長さに比例する定数である体系的なバイアスが生じる。バイアスは慎重な較正によって除去できるが、分散（または再現性）は測定方法の特性上の限界である。
<!-- The variance is the error in repeatability for a mcasurement; the bias is the
systcmatic crror duc to lack of calibration. For example, suppose that you
are measuring the length of a hallway with a yardstick, but instead of a
yardstick you accidentally pick up a meter stick. Your mcasurements would
still have the same repeatability, assuming that you were just as careful in
laying the measuring stick end to end as you measured the length of the hall.
But there would be a systematic bias, a constant proportional to the true
length of the hall, due to the incorrect length of the measuring stick. Bias
can be removed through careful calibration, but variance (or repeatability)
is a characteristic limitation of thc measurement method.-->
</p><p>
ヒストグラムは、測定値の分散とバイアスの両方を含む分布を調べるのに便利なツールである。実線上の測定値の範囲は、バケットと呼ばれる有限数の区間に分割される。バケットの数に等しい長さの1次元整数配列は、各区間に含まれる測定値の発生回数をカウントするために使用される。ヒストグラムをプロットすると、測定値の分布が示される。プロットの幅は分散を示し、分布の中心位置と真の測定値との差はバイアスを示す。
<!-- The histogram is a useful tool for sccing the distribution, including both
variance and bias, of a measurement. The range of measurements on the
real linc is partitioned into a finite number of intervals called buckets. A
one-dimensional integer array of length equal to the number of buckets is
used to count the number of occurrences of measurements that fall in the
intervals. A plot of the histogram shows the distribution of measurements.
The width of the plot is an indication of variance, and the difference between
the location of the center of the distribution and the true measurement is an
indication of bias.-->
</p><p>
ある量 \(x\) の測定値 \(y_i\) は、加法的な誤差によって壊れることがある。
<!-- A measurement \(y_i\) of some quantity \(x\) can be corrupted by additive error: -->
\[
y_i = x + e_i \tag{B.3}
\]
ここで、\(e_i\) は測定値の誤差である。もし誤差が既知であるか、較正によって誤差を除去できるのであれば、各測定値は未知の量の正確な推定値となり、それ以上の処理は必要ない。しかし、測定手順の再現性は完璧ではなく、各測定値には何らかの分布から導かれた誤差が含まれ、その結果、未知のパラメータに何らかの関連のある測定値の分布が生じる。統計学は測定手順に関する科学であり、誤差の特性に関する様々な仮定に基づいて未知のパラメータを推定する方法を含む。
<!-- where \(e_i\) is the error in the measuremcnt. If the error were known or if the
crror could be removed through calibration, then each measurement would
be an accurate estimate of the unknown quantity, and no further processing
would be necessary. However, the repeatability of a measurement procedure
is not perfect, and each measurement will include error drawn from some
distribution leading to a distribution of measurements somehow related to
the unknown parameter. Statistics is the science of measurement procedures
and includes methods for estituating unknown parameters given various as-
sumptions about the characteristics of the errors.-->
</p><p>
\(n\) 回の測定の平均は
<!-- The average of a set of n mcasurements is -->
\[
\overline{x}=\frac{1}{n}\sum_{i=1}^n x_i \tag{B.4}
\]
中央値は、測定値をソートし、中央の要素を選択する（または、測定値の数が偶数の場合は中央の2つの要素を平均する）ことによって計算される。モードは、測定値の分布におけるピークの位置である。平均値と中央値は、対称分布からの加法誤差によって乱された測定値から未知のパラメータを推定する方法である。例えば、局所的な近傍領域にわたってピクセルを平均化したり、一連の画像内の同じ位置で得られたピクセル値を平均化して、グレー値の測定値におけるノイズを低減したりすることができる。
<!-- The median is computed by sorting the mcasurements and choosing the mid-
dle element (or averaging the two middle elements if the number of measure-
ments is even). The mode is the location of the peak in the distribution
of measurements. The average and median are methods for estimating an
unknown parameter from measurements corrupted by additive errors from
a symmetric distribution. For example, pixels can be averaged over local
neighborhoods, or the pixel values obtained at the same location in a se-
quence of images can be averaged to reduce the noise in the measurements
of gray value.-->
</p><p>
誤差の大きさはいくつかの異なる方法で計算できる。平均絶対誤差（MAE）は\(\pm\delta_x\)で、
<!-- Several different measurements of the amount of error can be computed.
Mean absolute error (MAE) is \(\pm\delta_x\), where -->
\[
\delta_x=\frac{1}{n}\sum|x_i-\mu_x| \tag{B.5}
\]
\(\mu_x\)は平均値または中央値である。二乗平均平方根（RMS）誤差は\(\pm\delta_x\)で、
<!-- and \(\mu_x\) is the average or median. Root mean square (RMS) error is \(\pm\delta_x\), where -->
\[
\delta_x^2=\frac{1}{(n-1)}\sum(x_i-\mu_x)^2 \tag{B.6}
\]
\(\mu_x\)は平均である。最大誤差は\(\pm\epsilon_x\)である。
<!-- and \(\mu_x\) is the average. Maximum error is \(\pm\epsilon_x\), where -->
\[
\epsilon_x = \max_i |x_i-\mu_x| \tag{B.7}
\]
\(\mu_x\)は平均値または中央値である。\(\delta\leq \sigma\lt \epsilon\)であることに注意。
<!-- and \(\mu_x\) is the average or median. Note that \(\delta\leq \sigma\lt \epsilon\).-->
</p>
<h2>B.2. 誤差分布</h2>
<p>
二項分布は、事象と呼ばれる有限個の結果を伴う測定プロセスをモデル化する。例えば、公平なコインを1回投げると、確率1/2で表、確率1/2で裏が出る。この結果の集合は、次の多項式でモデル化される。
<!-- The binomial distribution models measurement processes with a finite num-
ber of outcomes, called events. For example, a fair coin that is flipped once
will show heads with probability 1/2 and tails with probability 1/2. This set
of outcomes is modeled by the polynomial -->
\[
P(x) = 0.5 + 0.5x \tag{B.8}
\]
ここで、\(x\)の各べき乗の係数は、表がその回数だけ出る確率である。コインを \(n\) 回投げた場合、表が出る確率は、多項式を展開することで次のように求めることができる。
<!-- where the coefficient, for each power of \(x\) is the probability that heads will
occur that many times. If the coin is tossed n times, then the probabilities of
various numbers of heads can be determined by expanding the polynomial:-->
\[
P(x;n) = (0.5+0.5x)^n \tag{B.9}
\]
表が \(i\) 回出る確率は、展開された多項式における \(x^i\) の係数である。一般に、\(k\) 回の結果を伴う単一の測定は、次数 \(k\) の多項式でモデル化できる。
<!-- The probability that heads will occur \(i\) times is the coefficient of \(x^i\) in the
expanded polynomial. In general, a single measurement with & outcomes can
be modeled by a polynomial of order \(k\),-->
\[
P)x)=p_0+p_1x+p_2x^2+\cdots+\_{k-1}x^{k-1} \tag{B.10}
\]
ここで \(p_i\) は結果 \(i\) の確率であり、
<!-- where \(p_i\) is the probability of outcome \(i\) and -->
\[
\sum_{i=1}^k p_i=1 \tag{B.11}
\]
一連の \(n\) 回の測定後の様々な結果の組み合わせの累積結果は、1回の測定の多項式を \(n\) 乗することでモデル化される。
<!-- The cumulative results of various combinations of outcomes after a sequence
of \(n\) measurements are modeled by raising the polynomial for one measurement to power \(n\),-->
\[
P(x;n)=(p_0+p_1x+p_2x^2+\cdots +p_{k-1}x^{k-1})^n \tag{B.12}
\]
多項式を展開し、各係数を計算する。
<!-- expanding the polynomial, and calculating each coefficient. -->
</p><p>
一様分布は、等確率の測定値をモデル化するために使用されます。例えば、ある点が、角にある点 \((x_1, y_1)\) と \((x_2, y_2)\) で定義される長方形領域内の任意の場所に等確率で存在する場合、その点が位置 \((x, y)\) にある確率は
<!--
The uniform distribution is used to model measurements that are equally
likely. For example, if a point is located anywhere with equal probability in
the rectangular region defined by points \((x_1, y_1)\) and \((x_2,y_2)\) at the corners,
then the probability that the point is at location \((x, y)\) is
-->
\[
P(x,y)=
\begin{cases}
1/A & \text{(x,y)が領域内にある場合} \\
0 & \text{そうではない場合} 
\end{cases}
\tag{B.13}
\]

ここで、\(A\) は領域の面積です。
</p><p>
正規分布は、多くの測定プロセスにおける誤差の有用な近似値です。正規分布 \(N(x; \mu, \sigma^2)\) は、
<!--
where \(A\) is the area of the region.
</p><p>
The normal distribution is a useful approximation to the crrors in many
measurement processes. The normal distribution \(N(x; \mu, \sigma^2)\) is
-->
\[
N(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}} \tag{B.14}
\]
ここで、\(\mu\) は分布の中心の位置、\(\sigma^2\) は分散です。
通常、適切な較正によってバイアスは除去されるため、誤差の正規分布は通常、平均ゼロとなります。
<!--
where \(\mu\) is the location of the center of the distribution and \(\sigma^2\) is the variance.
Since bias is usually eliminated by proper calibration, the normal distribution
for errors is usually zero mean.
-->
</p><p>
一部の測定プロセスでは、正規分布する誤差に加えて、外れ値と呼ばれる大きな誤差が時折発生します。このような誤差プロセスは、正規分布と未知の裾の広い分布の混合としてモデル化できます。
<!--
Some measurement processes occasionally produce gross errors, called
outliers, in addition to normally distributed errors. Such an error process
can be modeled as the mixture of a normal distribution and some unknown,
broad-tailed distribution:
-->
\[
(1-\nu)N(x;\mu_1,\sigma_1^2)+\nu B(x;\mu_2,\sigma_2^2) \tag{B.15}
\]
ここで、\(\nu\) は外れ値が発生する確率を表します。測定値が外れ値分布の誤差に汚染される確率は \(\nu\) であり、測定値が正規分布の誤差の影響を受ける確率は \(\nu - 1\) です。通常、両方の誤差プロセスによるバイアスは適切な較正によって除去されるため、両方の誤差分布は平均ゼロになります。
<!--
where \(\nu\) represents the odds that an outlier will occur. There is a probability
of \(\nu\) that the measurement will be contaminated with error from the outlier
distribution and a probability of \(\nu - 1\) that the measurement. will be subject
to normally distributed errors. Typically, the bias from both error processes
is eliminated through proper calibration, and so both error distributions are
zero mean:
-->
\[
(1 -\nu)N(x;0, \sigma_1^2) +\nu B(x;0, \sigma_2^2)  \tag{B.16}
\]

外れ値は極端な誤差なので、\(\sigma_2 \gg \sigma_1\)となる。例として、コーシー分布
<!--
Since outliers are extreme errors, \(\sigma_2 \gg \sigma_1\). As an example, the Cauchy
distribution
-->

\[
C(x;a,b)=\frac{1}{\pi}\frac{a}{a^2+(x-b)^2}  \tag{B.17}
\]
裾が非常に広いので、分散は無限大です。
<!--
has such broad tails that the variance is infinite.
-->
</p>
<h2>B.3 線形回帰</h2>
<!--
<h2>B.3 Linear Regression</h2>
-->
<p>
\(n\)個のデータ点と\(m\)個のパラメータ\(a_1, a_2, ..., a_m,\)を持つモデルが与えられた場合、モデルをデータに適合させる際の最小二乗誤差は
<!--
Given \(n\) data points and a model with \(m\) parameters \(a_1, a_2, ..., a_m,\) the least-
squares error in the fit of the model to the data is
-->
\[
\chi^2=\sum_{i=1}^n \left(\frac{y_i-y(x_i;a_1,a_2,...,a_m)}{\sigma_i}\right)^2 \tag{B.18}
\]

これは重み付き最小二乗回帰であり、重み \(\sigma_i\) は測定値の誤差（標準偏差）であり、ノイズの少ない測定値に重みが付けられます。
<!--
This is weighted least-squares regression, where the weights \(sigma_i\) are the errors
(standard deviations) in the measurements so that less noisy measurements
are given more weight.
-->
</p><p>
多くの場合、誤差は各測定で同じであるか、個々の測定誤差が不明で同一であると仮定されます。その場合、最小二乗回帰問題は、次の式を最小化するパラメータ \(a_1,a_2,...,a_m\) を決定することです。
<!--
Often, the error is the same for each measurement, or the individual mea-
surement errors are unknown and assumed to be identical, in which case the
least-squares regression problem is to determine the parameters \(a_1,a_2,...,a_m\)
that minimize
-->
\[
\chi^2=\sum_{i=1}^n (y_i-y(x_i;a_1,a_2,...,a_m))^2 \tag{B.19}
\]

モデルがパラメータに関して線形である場合、問題は線形回帰です。モデルが線形であるとは、基底関数の線形結合として表せる場合です。
<!--
If the model is linear in the parameters, then the problem is linear regression.
A model is linear if it can be represented as a linear combination of basis
functions:
-->
\[
y(x;a_1,a_2,...,a_m) = a_1\phi_1(x)+a_2\phi_2(x+\cdots+a_m\phi_m(x)  \tag{B.20}
\]

ここで、関数\(\phi_i(x)\)はモデルパラメータに依存しません。線形結合の係数\(a_1,a_2,...,a_n\)は回帰によって決定されるモデルパラメータです。例えば、次の直線は
<!--
where the functions \(\phi_i(x)\) do not depend on the model parameters. The coef-
ficients \(a_1,a_2,...,a_n\) of the linear combination are the model parameters
to be determined through regression. For example, the line
-->
\[
y=ax+b  \tag{B.21}
\]
は、パラメータ \(a\) と \(b\) を持つ線形モデルです。
<!--
is a linear model with parameters \(a\) and \(b\).
-->
</p><p>

線形最小二乗回帰問題は、特異値分解のための標準的な数値ルーチン [84, pp. 534-539] を用いて確実に解くことができ、これにより、近似パラメータの誤差の尺度も得られる。線形モデルの近似について考える。
<!--
A linear least-squares regression problem can be reliably solved using stan-
dard numerical routines for singular value decomposition [84, pp. 534 539],
which also provides a measure of the error in the fitted parameters. Consider
fitting a linear model
-->
\[
z=a_1+a_2x+a_3y  \tag{B.22}
\]

データポイントの集合\(\{(x_1,y_1,z_1), (x_2,y_2,z_2),...,(x_n,y_n,z_n)\}\)。各データポイントは制約につながる。
<!--
to a set of data points \(\{(x_1,y_1,z_1), (x_2,y_2,z_2),...,(x_n,y_n,z_n)\}\). Each data
point leads to a constraint
-->

\[
z_i \approx a_1+a_2x_i+a_3y_i  \tag{B.23}
\]

そして、データポイントの集合は、行列形式で記述できる制約の集合につながります。
<!--
and the set of data points leads to a set of constraints that can be written in
matrix form:
-->

\[
\begin{bmatrix}
1 & x_1 & y_1 \\
1 & x_2 & y_2 \\
& \vdots & \\
1 & x_n & y_n
\end{bmatrix}

\begin{bmatrix}
a_1 \\
a_2 \\
a_3
\end{bmatrix}
=
\begin{bmatrix}
z_1 \\
z_2 \\
\vdots \\
z_n
\end{bmatrix}
\tag{B.24}
\]

これらの方程式は通常、より簡潔な形式で書かれる。
<!--
These equations are usually written in the more compact form
-->
\[
AX = B  \tag{B.25}
\]

\(A\)行列は正方行列ではないため、直接逆行列を求めることはできません。一つの方法は、式B.25の両辺に\(A^T\)を掛けて正規方程式を作成し、LU分解などの連立一次方程式を解く標準的な手法を用いて正規方程式を解くことです。より優れた方法は、特異値分解を使用することです。特異値分解にはいくつかの利点があります。(1)正規方程式を作成するために式B.25を事前に掛け合わせる必要がないこと、(2)特異値分解は条件の悪い連立方程式を扱うことができること、(3)SVDの副産物として得られる特異値は、モデル内の冗長性（不要な項）を示すことです。
<!--
The \(A\) matrix is not square and cannot be inverted directly. One technique is
to multiply both sides of Equation B.25 by \(A^T\) to form the normal equations
and solve the normal equations using standard techniques for solving systems
of linear equations such as LU decomposition. A better technique is to use
singular value decomposition. There are several advantages to using singular
value decomposition: (1) it is not necessary to premultiply Equation B.25
to form the normal equations, (2) singular value decomposition handles ill-
conditioned systems of equations, and (3) the singular values provided as a
by-product of SVD indicate redundancies (unnecessary terms) in the model.
-->

</p><p>

特異値分解は本書のアルゴリズムで頻繁に用いられており、またNumerical Recipes [197]は優れた数値アルゴリズムの優れた情報源であるため、本書の特異値分解ルーチンを用いて線形回帰問題を解く方法を詳細に示す。（Cプログラミングのいくつかの要素は付録Cで説明されていることに注意してください。）特異値分解の数値ルーチンのインターフェースは
<!--
Since singular value decomposition is used frequently for the algorithms
in this book and Numerical Recipes [197] is an excellent source for good
numerical algorithms, the method for solving a linear regression problem
using the singular value decomposition routine in Numerical Recipes will
be presented in detail. (Note that some elements of C programming are
described in Appendix C.} The interface for the numerical routine for singular
value decomposition is
-->
</p><p class="margin-large">
<code>
void svdcmp (a, n, m, w, v)<br>
float **a, *w, **v;<br>
int n, m;<br>
</code>
</p><p>

このルーチンは、n行m列の配列aを受け取り、それを特異値分解で置き換えます。
<!--
The routine takes an array a with n rows and m columns and replaces it with
the singular value decomposition
-->

\[
A=UWV^T   \tag{B.26}
\]

特異値分解において、配列 \(a\) は \(U\) に置き換えられ、配列 \(w\) は対角行列 \(W\) の特異値を受け取り、配列 \(v\) は \(V\) 行列を受け取ります。\(n\) は観測値（測定値）の数であり、\(m\) は線形モデルのパラメータの数であることに注意してください。式 B.24 に従って配列 a の要素を設定し、svdcmp を呼び出して特異値分解を得るのは簡単です。特異値分解が得られたら、ルーチン svbksb を使用してモデルパラメータを解きます。
<!--
The array \(a\) is replaced by \(U\) in the singular value decomposition, the array \(w\)
receives the singular values in the diagonal matrix \(W\), and the array \(v\) receives
the \(V\) matrix. Note that \(n\) is the number of observations (measurements),
while \(m\) is the number of parameters in the linear model. It is easy to fill
the entries in the array a according to Equation B.24 and call svdcmp to
obtain the singular value decomposition. After obtaining the singular value
decomposition, use the routine svbksb to solve for the model parameters:
-->

</p><p class="margin-large">
<code>
void svbksb (u, w, v, n, m, b, x)<br>
float **u, *w, *#V, ¥D, ¥X;<br>
int n, m;<br>
</code>
</p><p>

配列 \(u, w,\) と \(v\) は、svdemp によって計算される配列 \(a, w,\) と \(v\) です。次元 \(n\) と \(m\) は svdemp と同じです。配列 b は式 B.24 の右辺であり、配列 \(x\) はパラメータベクトル（解）です。言い換えれば、svdcmp と svbksb の組み合わせは、式 B.24 で示される非正方線形連立方程式を解きます。これらのルーチンを呼び出すコードは次のとおりです。
<!--
The \(u, w,\) and \(v\) arrays are the \(a, w,\) and \(v\) arrays computed by svdemp. The
dimensions \(n\) and \(m\) are the same as for svdemp. The b array is the right-hand
side of Equation B.24, and the array \(x\) is the parameter vector (solution).
In other words, the combination of svdcmp and svbksb solve the nonsquare
system of lincar equations presented as Equation B.24. The code fragment
for invoking the routines is
-->
</p><p class="margin-large">

<code>
float wmin, wmax;<br>
svdemp (a, n, m, w, v);<br>
wmax = 0.0;<br>
for (j = 1; j <= m; j++)<br>
　if (w[j] > wmax) wmax = w[j];<br>
wmin = wmax * 1.0e-6;<br>
for (j = 1; j <= m; j++)<br>
　if (wlj] < wmin) wfj] = 0.0;<br>
svbksb (a, w, v, n, m, b, x);<br>
</code>
</p><p>

小さな特異値は回帰モデルに問題があることを示しています。上記のコードでは、小さな特異値をゼロに設定していますが、これは問題に対処する安全な方法の一つです。定数1.0e-6は典型的な値ですが、アプリケーションによっては異なる値になる場合があります。小さな特異値がある場合は、モデルを注意深く分析し、小さな特異値が発生する理由を特定することが重要です。モデルには不要な項が含まれている可能性があります。
<!--
Small singular values indicate problems in the regression model. The code
provided above sets small singular values to zero, which is one safe way to
handle the problem. The constant 1.0e-6 is a typical value but may be dif
ferent for some applications. If there are small singular valucs, it is important
to carefully analyze the model and determine why the small singular values
occur. There may be terms in the model that arc unnecessary.
-->
</p><p>
特異値分解は非常に信頼性の高いアルゴリズムですが、いくつかの理由により、良好な回帰推定値が得られない場合があります。
<!--
Singular value decomposition is a very reliable algorithm but can fail to
produce a good regression estimate for several reasons:
-->
<div class="styleBullet">
<ul><li>
● 回帰問題の定式化に誤ったモデルが使用されている可能性があります。

</li><br><li>● 測定誤差 \(\sigma_i\) が大きすぎる可能性があります。

</li><br><li>● 測定誤差は正規分布に由来しない可能性があります。例えば、誤差は裾野の広い分布に由来する可能性があります。
<!--
● The wrong model may be used in formulating the regression problem.

</li><br><li>● The measurement errors \(\sigma_i\) may be too large.

</li><br><li>● The measurement errors may not be from a normal distribution. For
example, the errors could be from a broad-tailed distribution.
-->
</li></ul></div>
</p><p>
回帰ノルムの最小値が得られる\(\chi^2\)の確率分布は、自由度\(\nu = n - m\)のカイ二乗分布である。カイ二乗が偶然に\(\chi^2\)を超える確率は、
<!--
The probability distribution for \(\chi^2\) when the minimum value of the regres-
sion norm is obtained is the chi-square distribution with \(\nu = n - m\) degrees
of freedom. The probability that chi-square should exceed \(\chi^2\) by chance is
-->
\[
Q=\frac{1}{\Gamma\left(\frac{\nu}{2}\right)} \int_{\chi^2}^\infty e^{-t}t^{\frac{\nu}{2}-1}dt \tag{B.27}
\]

回帰誤差 \(\chi^2\) は、回帰手順の一部として計算する必要があります。積分は数値計算法を用いて計算できますが、\(Q\) の値は統計表で提供されています。\(Q > 0.001\) の場合、回帰近似は棄却されるべきでしょう。これは、モデル近似手順を客観的に評価する方法を提供します。実際には、\(Q\) の値は、適用例に関する知識に基づいて選択されます。\(\nu\) の値は、データ点の数とモデルの次数によって決定されます。対応する \(\chi^2\) の値は、統計表で見つけることができます。回帰中に得られた \(\chi^2\) の測定値が表の値を超える場合、アルゴリズムは成功していないため、結果は破棄する必要があります。 \(\chi^2\) の値が過度に高い場合が頻繁に発生する場合、モデルが間違っているか、線形回帰が適切なアプローチではない可能性があり、代わりにロバスト回帰を使用する必要があると考えられます。
<!--
The regression error \(\chi^2\) should be calculated as part of the regression pro-
cedure. The integral can be calculated using numerical methods, but values
of \(Q\) are provided in statistical tables. If \(Q > 0.001\), then the regression fit
should probably be rejected. This provides an objective way to evaluate the
model fitting procedure. In practice, the value for \(Q\) is selected based on
knowledge of the application. The value for \(\nu\) is determined by the number
of data points and the order of the model. The corresponding value for \(\chi^2\)
can be found in statistical tables. If the measured value for \(\chi^2\) obtained
during regression exceeds the tabulated value, then the algorithm has not
succeeded, and the results should be discarded. If excessive values for \(\chi^2\) are
encountered frequently, then the model is probably wrong or linear regres-
sion is not the right approach, and perhaps robust regression should be used
instead.
-->
</p>
<h2>B.4 非線形回帰</h2>
<!--
<h2>B.4 Nonlinear Regression</h2>
-->
<p>
モデルのパラメータが非線形である場合、最小二乗回帰問題は、非線形モデルにおける\(\chi^2\)を最小化することである。モデルの式は既知であるため、勾配とヘッセ行列の両方を計算できる。これにより、Levenberg-Marquardt法を非線形回帰問題に用いることができる[197, pp. 540-547]。
<!--
If the model is nonlinear in its parameters, the least-squares regression prob-
lem is to minimize \(\chi^2\) for the nonlincar modcl. Since the formula for the
model is known, both the gradient and Hessian can be calculated. This
allows the Levenberg-Marquardt method to be used for selving nonlinear
regression problems [197, pp. 540-547].
-->
</p><p>
SeberとWild [217]は非線形回帰に関する優れた教科書です。Numerical RecipesのLevenberg-Marquardtルーチンの引数が意図した用途と一致しない場合もありますが、Numerical RecipesにはNewton-Raphson法のルーチンも用意されており、これは非線形回帰の実用教科書[23]で解説されています。
<!--
Seber and Wild [217] is an excellent text on nonlinear regression. In
some cases, the arguments to the Levenberg-Marquardt routine in Numeri-
cal Recipes may not match the intended application, but Numerical Recipes
provides routines for the Newton-Raphson method, which is discussed in
practical texts on nonlinear regression [23].
-->
</p>
<h2>さらに学ぶために</h2>
<!--
<h2>Further Reading</h2>
-->
<p>
確率と統計に関する優れた教科書は数多くあります。科学や工学における実験手法、あるいは社会科学における統計手法を解説した書籍であれば、どれでも十分です。例えば、BeckとArnold [24]は、工学と科学への応用に重点を置きながら、回帰分析における線形法と非線形法を解説しています。Box、Hunter、Hunter [43]は、実験手法に関する古典的な教科書を著しています。Drake [69]は確率と統計の基礎的な入門書を著し、Papoulis [192]はより包括的な解説を提供しています。ガウス誤差モデルは通信理論 [257] で使用されており、機械視覚やパターン認識に影響を与えています。Vanmarcke [244]とCressie [63]は、空間データに対する確率統計手法を紹介しており、機械視覚に関する更なる研究に役立つかもしれません。ロバスト回帰法は、EfronとTibshirani [72]の論文にまとめられています。最後に、CのNumerical Recipesは統計アルゴリズムの優れた情報源であり[197]、特に回帰分析をわかりやすく説明しています。
<!--
There are many excellent textbooks on probability and statistics. Any book
that describes experimental methods in science and engineering or statistica
methods in the social sciences would be sufficient. For example, Beck and
Arnold [24] cover linear and nonlinear methods for regression with emphasis
on applications in engineering and science. Box, Hunter, and Hunter [43
have written a classic text on experimental methods. Drake [69] has writ-
ten a basic introduction to probability and statistics, while Papoulis [192
provides more comprehensive coverage. Gaussian error models are used in
communications theory [257], which has influenced machine vision and pat-
tern recognition. Vanmarcke [244] and Cressie [63] present probability and
statistical methods for spatial data which may be useful as further readings
in machine vision. Robust regression methods are summarized in the article
by Efron and Tibshirani [72]. Finally, Numerical Recipes in Cis an excellen
source for statistics algorithms [197] and explains regression particularly well.
-->
</p>
    </body>
</html>